{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afcb7a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "csv_chunk_embed_qdrant.py\n",
    "\n",
    "- Reads CSV files in `data_dir`.\n",
    "- Uses a specified text column to create chunked documents.\n",
    "- Creates embeddings using a HuggingFace sentence-transformers model.\n",
    "- Stores vectors in Qdrant with metadata (filename, row_index, chunk_index, text).\n",
    "- Allows interactive similarity search.\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import math\n",
    "import uuid\n",
    "from typing import List, Dict, Iterable\n",
    "import pandas as pd\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from qdrant_client import QdrantClient\n",
    "from qdrant_client.http.models import VectorParams, Distance, PointStruct\n",
    "from tqdm import tqdm\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3672661",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ---------- USER CONFIG ----------\n",
    "DATA_DIR = \"data\"      # directory containing *.csv\n",
    "TEXT_COLUMN = \"text\"       # column name to read from CSVs (change if needed)\n",
    "MODEL_NAME = \"sentence-transformers/all-MiniLM-L6-v2\"  # HF sentence-transformers model\n",
    "COLLECTION_NAME = \"csv_chunks\"\n",
    "CHUNK_MAX_WORDS = 120      # max words per chunk\n",
    "CHUNK_OVERLAP_WORDS = 20   # overlap between adjacent chunks\n",
    "BATCH_SIZE = 64            # embedding batch size\n",
    "QDRANT_HOST = \"localhost\"  # change if remote\n",
    "QDRANT_PORT = 6333\n",
    "# ---------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11a81ed9",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def chunk_text(text: str, max_words: int = 120, overlap: int = 20) -> List[str]:\n",
    "    \"\"\"Split text into chunks by words with overlap.\"\"\"\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return []\n",
    "    words = text.split()\n",
    "    if len(words) <= max_words:\n",
    "        return [\" \".join(words)]\n",
    "    chunks = []\n",
    "    start = 0\n",
    "    while start < len(words):\n",
    "        end = start + max_words\n",
    "        chunk_words = words[start:end]\n",
    "        chunks.append(\" \".join(chunk_words))\n",
    "        if end >= len(words):\n",
    "            break\n",
    "        start = end - overlap  # overlap\n",
    "    return chunks\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28bb71df",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def iter_csv_rows(data_dir: str, text_column: str):\n",
    "    \"\"\"Yield tuples (filename, row_index, text) for every row in CSVs found.\"\"\"\n",
    "    for fname in os.listdir(data_dir):\n",
    "        if not fname.lower().endswith(\".csv\"):\n",
    "            continue\n",
    "        path = os.path.join(data_dir, fname)\n",
    "        try:\n",
    "            df = pd.read_csv(path, dtype=str, keep_default_na=False)  # read as str to avoid NaNs\n",
    "        except Exception as e:\n",
    "            print(f\"Failed to read {path}: {e}\")\n",
    "            continue\n",
    "        if text_column not in df.columns:\n",
    "            print(f\"Warning: {fname} missing column '{text_column}', skipping.\")\n",
    "            continue\n",
    "        for idx, row in df.iterrows():\n",
    "            text = row.get(text_column, \"\")\n",
    "            yield fname, idx, text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a91011b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def embed_batches(model: SentenceTransformer, texts: Iterable[str], batch_size: int = 64):\n",
    "    \"\"\"Yield embeddings for batches of texts.\"\"\"\n",
    "    texts = list(texts)\n",
    "    for i in range(0, len(texts), batch_size):\n",
    "        batch = texts[i:i+batch_size]\n",
    "        emb = model.encode(batch, show_progress_bar=False, batch_size=len(batch), convert_to_numpy=True)\n",
    "        yield i, batch, emb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be06b6c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) load model\n",
    "print(\"Loading embedding model:\", MODEL_NAME)\n",
    "model = SentenceTransformer(MODEL_NAME)\n",
    "emb_dim = model.get_sentence_embedding_dimension()\n",
    "print(\"Embedding dimension:\", emb_dim)\n",
    "\n",
    "# 2) connect to Qdrant\n",
    "print(\"Connecting to Qdrant at\", f\"{QDRANT_HOST}:{QDRANT_PORT}\")\n",
    "client = QdrantClient(host=QDRANT_HOST, port=QDRANT_PORT, prefer_grpc=False)\n",
    "\n",
    "# 3) create collection (if not exists)\n",
    "existing_collections = [c.name for c in client.get_collections().collections]\n",
    "\n",
    "if COLLECTION_NAME not in existing_collections:\n",
    "    print(\"Creating collection:\", COLLECTION_NAME)\n",
    "    client.recreate_collection(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        vectors_config=VectorParams(size=emb_dim, distance=Distance.COSINE),\n",
    "    )\n",
    "else:\n",
    "    print(\"Collection exists:\", COLLECTION_NAME)\n",
    "\n",
    "# 4) iterate CSV rows, chunk and prepare records\n",
    "texts_for_embedding = []\n",
    "mapping = []\n",
    "\n",
    "print(\"Reading CSVs and chunking texts...\")\n",
    "\n",
    "for fname, row_idx, text in iter_csv_rows(DATA_DIR, TEXT_COLUMN):\n",
    "    chunks = chunk_text(text, CHUNK_MAX_WORDS, CHUNK_OVERLAP_WORDS)\n",
    "\n",
    "    for chunk_idx, chunk in enumerate(chunks):\n",
    "        meta = {\n",
    "            \"source_file\": fname,\n",
    "            \"row_index\": row_idx,\n",
    "            \"chunk_index\": chunk_idx,\n",
    "            \"text\": chunk[:1000],\n",
    "        }\n",
    "\n",
    "        texts_for_embedding.append(chunk)\n",
    "        mapping.append(meta)\n",
    "\n",
    "if not texts_for_embedding:\n",
    "    print(\"No text chunks found. Check DATA_DIR and TEXT_COLUMN.\")\n",
    "else:\n",
    "    print(f\"Total chunks to embed: {len(texts_for_embedding)}\")\n",
    "\n",
    "    # 5) embed in batches and upsert to Qdrant\n",
    "    print(\"Embedding and uploading vectors in batches...\")\n",
    "\n",
    "    next_id = 0\n",
    "\n",
    "    for start_idx, batch_texts, batch_emb in embed_batches(\n",
    "        model, texts_for_embedding, BATCH_SIZE\n",
    "    ):\n",
    "        points_batch = []\n",
    "\n",
    "        for j, emb in enumerate(batch_emb):\n",
    "            meta = mapping[start_idx + j]\n",
    "            point_id = str(uuid.uuid4())\n",
    "\n",
    "            p = PointStruct(\n",
    "                id=point_id,\n",
    "                vector=emb.tolist(),\n",
    "                payload=meta,\n",
    "            )\n",
    "\n",
    "            points_batch.append(p)\n",
    "\n",
    "        client.upsert(\n",
    "            collection_name=COLLECTION_NAME,\n",
    "            points=points_batch,\n",
    "        )\n",
    "\n",
    "        next_id += len(points_batch)\n",
    "        print(f\"Upserted {next_id} vectors...\", end=\"\\r\")\n",
    "\n",
    "    print(f\"\\nFinished upserting {next_id} vectors into '{COLLECTION_NAME}'.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9bc1b150",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 6) interactive search\n",
    "print(\"\\nIndexing complete. You can now enter queries. Type 'exit' to quit.\")\n",
    "\n",
    "while True:\n",
    "    query = input(\"\\nEnter query: \").strip()\n",
    "\n",
    "    if not query:\n",
    "        continue\n",
    "\n",
    "    if query.lower() in (\"exit\", \"quit\"):\n",
    "        break\n",
    "\n",
    "    q_emb = model.encode([query], convert_to_numpy=True)[0]\n",
    "\n",
    "    response = client.query_points(\n",
    "        collection_name=COLLECTION_NAME,\n",
    "        query=q_emb.tolist(),\n",
    "        limit=10,\n",
    "    )\n",
    "\n",
    "    hits = response.points\n",
    "\n",
    "    if not hits:\n",
    "        print(\"No results.\")\n",
    "        continue\n",
    "\n",
    "    for rank, hit in enumerate(hits, start=1):\n",
    "        payload = hit.payload or {}\n",
    "        score = hit.score\n",
    "        preview = payload.get(\"text\", \"\")[:500]\n",
    "\n",
    "        print(\n",
    "            f\"\\n{rank}. score={score:.4f} â€” \"\n",
    "            f\"file={payload.get('source_file')} \"\n",
    "            f\"row={payload.get('row_index')} \"\n",
    "            f\"chunk={payload.get('chunk_index')}\"\n",
    "        )\n",
    "\n",
    "        print(preview)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
